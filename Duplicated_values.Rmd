

# Duplicated Values
In many cases, we observe duplicated values in a data set where every instances are supposed to be unique. We will now discussed how to handle duplicates.

## Data set with row-wise duplicates
Although the dataset we use in this example, airquality data, does not have duplicate, we will use it illustrate the techniques of handling duplicate data.
```{r, echo = TRUE}
data_duplicated_values = airQuality_preProcessing
```
We see the data set should have no duplicate to begin with.
```{r, echo = TRUE}
sum(duplicated(data_duplicated_values))
```

Now we randomly pick 5 instances from the dataset to induce duplication
```{r, , echo = TRUE}

for (i in 1: 5)
{
  data_duplicated_values[nrow(data_duplicated_values)+1,] = data_duplicated_values[floor(runif(1, min = 1, max = nrow(data_duplicated_values))),]
}

```
We should now have 5 duplicated instances:

```{r, echo = TRUE}
sum(duplicated(data_duplicated_values))
```

The "duplicated(df)" returns a boolean array where each value at each index indicates if row at the same index in original data set is duplicated or not. We can use 'duplicated(df)" to extract duplicated rows:
```{r, echo = TRUE}
data_duplicated_values[duplicated(data_duplicated_values),]
```

By adding "!" before "duplicated(df)", we can negate logics in "duplicated(df)" and access non duplicate rows as a data frame:
```{r, echo = TRUE}
head(data_duplicated_values[!duplicated(data_duplicated_values),], 10)
```
There should be no duplicates

```{r, echo = TRUE}
sum(duplicated(data_duplicated_values[!duplicated(data_duplicated_values),]))
```

We can create a new reference to the data set with no duplicates. For the purpose of reusing "data_duplicated_values", I will just assign the new reference to itself:
```{r, echo = TRUE}
data_duplicated_values = data_duplicated_values[!duplicated(data_duplicated_values),]

```
Now "data_duplicated_values" should have no duplicates:
```{r, echo = TRUE}
sum(duplicated(data_duplicated_values))

```

Another way is to use "distinct()" function from tidyverse package.
```{r, echo = TRUE}
for (i in 1: 5)
{
  data_duplicated_values[nrow(data_duplicated_values)+1,] = data_duplicated_values[floor(runif(1, min = 1, max = nrow(data_duplicated_values))),]
}
sum(duplicated(data_duplicated_values))
data_duplicated_values <- data_duplicated_values %>% distinct()
sum(duplicated(data_duplicated_values))

```

## Duplicates based on specific columns
Some times duplication of elements in certain column/columns is not desirable. We want to be able to remove rows with duplication in specified columns. We first insert 5 rows with duplicates in "Day" and "Month", we should see "158" rows after insertion:
```{r, echo = TRUE}
for (i in 1: 5)
{
  Ozone <- floor(runif(1,min = 0, max = 50))
  Solor <- floor(runif(1,min = 0, max = 300))
  Wind <- round(runif(1,min = 0, max = 20), 2)
  Temp <- floor(runif(1,min = 0, max = 20))
  
  random_index = floor(runif(1, min = 1, max = nrow(data_duplicated_values)))
  
  Month <- data_duplicated_values[random_index, 'Month']
  Day <- data_duplicated_values[random_index, 'Day']
  data_duplicated_values[nrow(data_duplicated_values)+1,] = c(Ozone, Solor, Wind, Temp, Month, Day)
  
}
nrow(data_duplicated_values)


```

We can remove rows with duplicated "Day" and "Month" combination using "distinct()". The ".keep_all = TRUE" argument makes sure that for each duplicated combination of Day and Month, we keep the first row for every duplicated combination and we keep all variables in the Data. We should observe the number of rows after dropping dupicated columns going back to "153", which is the size of data frame before we insert rows with duplicated "Day" and "Month" combinations.
```{r, echo = TRUE}
data_duplicated_values<- data_duplicated_values %>% distinct(Day, Month, .keep_all = TRUE)
nrow(data_duplicated_values)
```

## Conclusion
We can use "distinct" to remove duplicated rows based on specified columns in data frame. However, we should only drop duplicated columns if we know there should be no duplicated columns in dataset and duplications are likely result of error in data collection.
